{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import init\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_kaiming(m):\n",
    "    \"\"\"Initialize weights according to method describe here:\n",
    "    https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConv(Module):\n",
    "\n",
    "    def __init__(self, in_feat, out_feat, drop_rate=0.4, bn_momentum=0.1, is_2d=True):\n",
    "        super(DownConv, self).__init__()\n",
    "        if is_2d:\n",
    "            conv = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "            dropout = nn.Dropout2d\n",
    "        else:\n",
    "            conv = nn.Conv3d\n",
    "            bn = nn.InstanceNorm3d\n",
    "            dropout = nn.Dropout3d\n",
    "\n",
    "        self.conv1 = conv(in_feat, out_feat, kernel_size=3, padding=1)\n",
    "        self.conv1_bn = bn(out_feat, momentum=bn_momentum)\n",
    "        self.conv1_drop = dropout(drop_rate)\n",
    "\n",
    "        self.conv2 = conv(out_feat, out_feat, kernel_size=3, padding=1)\n",
    "        self.conv2_bn = bn(out_feat, momentum=bn_momentum)\n",
    "        self.conv2_drop = dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.conv1_drop(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv2_bn(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(Module):\n",
    "\n",
    "    def __init__(self, in_feat, out_feat, drop_rate=0.4, bn_momentum=0.1, is_2d=True):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.is_2d = is_2d\n",
    "        self.downconv = DownConv(in_feat, out_feat, drop_rate, bn_momentum, is_2d) # chm\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # For retrocompatibility purposes\n",
    "        if not hasattr(self, \"is_2d\"):\n",
    "            self.is_2d = True\n",
    "        mode = 'bilinear' if self.is_2d else 'trilinear'\n",
    "        dims = -2 if self.is_2d else -3\n",
    "        x = F.interpolate(x, size=y.size()[dims:], mode=mode, align_corners=True)\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.downconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it in the code \n",
    "class _GridAttentionBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3,\n",
    "                 sub_sample_factor=2):\n",
    "        super(_GridAttentionBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [2, 3] # for debugging\n",
    "\n",
    "        # Downsampling rate for the input featuremap\n",
    "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
    "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
    "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
    "\n",
    "        # Default parameter set\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels (pixel dimensions)\n",
    "        self.in_channels = in_channels \n",
    "        self.gating_channels = gating_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            bn = nn.BatchNorm3d\n",
    "            self.upsample_mode = 'trilinear'\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "            self.upsample_mode = 'bilinear'\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # Output transform\n",
    "        self.W = nn.Sequential(\n",
    "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
    "            bn(self.in_channels),\n",
    "        )\n",
    "\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
    "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Initialise weights\n",
    "        for m in self.children():\n",
    "            weights_init_kaiming(m)\n",
    "         \n",
    "        # Define the operation\n",
    "        self.operation_function = self._concatenation\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :param g: (b, g_d)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        output = self.operation_function(x, g)\n",
    "        return output\n",
    "\n",
    "    def _concatenation(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
    "        # phi   => (b, g_d) -> (b, i_c)\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        f = F.relu(theta_x + phi_g, inplace=True)\n",
    "\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
    "\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n",
    "\n",
    "\n",
    "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None,\n",
    "                 sub_sample_factor=2):\n",
    "        super(GridAttentionBlock2D, self).__init__(in_channels,\n",
    "                                                   inter_channels=inter_channels,\n",
    "                                                   gating_channels=gating_channels,\n",
    "                                                   dimension=2,\n",
    "                                                   sub_sample_factor=sub_sample_factor,\n",
    "                                                   )\n",
    "\n",
    "\n",
    "class GridAttentionBlock3D(_GridAttentionBlockND):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None,\n",
    "                 sub_sample_factor=(2,2,2)):\n",
    "        super(GridAttentionBlock3D, self).__init__(in_channels,\n",
    "                                                   inter_channels=inter_channels,\n",
    "                                                   gating_channels=gating_channels,\n",
    "                                                   dimension=3,\n",
    "                                                   sub_sample_factor=sub_sample_factor,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnetGridGatingSignal2(nn.Module):\n",
    "    \"\"\"Operation to extract important features for a specific task using 1x1 convolution (Gating) which is used in the 2D\n",
    "    attention blocks.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): Number of channels in the input image.\n",
    "        out_size (int): Number of channels in the output image.\n",
    "        kernel_size (tuple): Convolution kernel size.\n",
    "        is_batchnorm (bool): Boolean indicating whether to apply batch normalization or not.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (Sequential): 2D convolution, batch normalization and ReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size, out_size, kernel_size=(1, 1), is_batchnorm=True):\n",
    "        super(UnetGridGatingSignal2, self).__init__()\n",
    "\n",
    "        if is_batchnorm:\n",
    "            \n",
    "            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size, (1, 1), (0, 0)),\n",
    "                                       nn.BatchNorm2d(out_size),\n",
    "                                       nn.ReLU(inplace=True)\n",
    "                                       )\n",
    "        else:\n",
    "                                       \n",
    "            self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size, (1, 1), (0, 0)),\n",
    "                                       nn.ReLU(inplace=True)\n",
    "                                       )\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            weights_init_kaiming(m)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "\n",
    "    def __init__(self, in_channel=1, depth=3, drop_rate=0.4, bn_momentum=0.1, n_metadata=None, film_layers=None,\n",
    "                 is_2d=True, n_filters=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.down_path = nn.ModuleList()\n",
    "        # first block\n",
    "        self.down_path.append(DownConv(in_channel, n_filters, drop_rate, bn_momentum, is_2d))\n",
    "        self.down_path.append(FiLMlayer(n_metadata, n_filters) if film_layers and film_layers[0] else None)\n",
    "        max_pool = nn.MaxPool2d if is_2d else nn.MaxPool3d\n",
    "        self.down_path.append(max_pool(2))\n",
    "\n",
    "        for i in range(depth - 1):\n",
    "            self.down_path.append(DownConv(n_filters*2**i, n_filters*2**(i+1), drop_rate, bn_momentum, is_2d))\n",
    "            self.down_path.append(FiLMlayer(n_metadata, n_filters * 2**(i+1)) if film_layers and film_layers[i + 1] else None)\n",
    "            self.down_path.append(max_pool(2))\n",
    "\n",
    "        # Bottom\n",
    "        self.conv_bottom = DownConv(n_filters*2**(depth-1), n_filters*2**depth, drop_rate, bn_momentum, is_2d)# change by me\n",
    "        self.film_bottom = FiLMlayer(n_metadata, n_filters*2**depth) if film_layers and film_layers[self.depth] else None #changeby me\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        features = []\n",
    "\n",
    "        # First block\n",
    "        x = self.down_path[0](x)\n",
    "        if self.down_path[1]:\n",
    "            x, w_film = self.down_path[1](x, context, None)\n",
    "        features.append(x)\n",
    "        x = self.down_path[2](x)\n",
    "\n",
    "        # Down-sampling path (other blocks)\n",
    "        for i in range(1, self.depth):\n",
    "            x = self.down_path[i * 3](x)\n",
    "            if self.down_path[i * 3 + 1]:\n",
    "                x, w_film = self.down_path[i * 3 + 1](x, context, None if 'w_film' not in locals() else w_film)\n",
    "            features.append(x)\n",
    "            x = self.down_path[i * 3 + 2](x)\n",
    "\n",
    "        # Bottom level\n",
    "        x = self.conv_bottom(x)\n",
    "        if self.film_bottom:\n",
    "            x, w_film = self.film_bottom(x, context, None if 'w_film' not in locals() else w_film)\n",
    "        features.append(x)\n",
    "        return features, None if 'w_film' not in locals() else w_film\n",
    "\n",
    "\n",
    "class Decoder(Module):\n",
    "\n",
    "\n",
    "    def __init__(self, out_channel=1, depth=3, drop_rate=0.4, bn_momentum=0.1,\n",
    "                 n_metadata=None, film_layers=None, hemis=False, final_activation=\"sigmoid\", is_2d=True,\n",
    "                 n_filters=64, attention= True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.out_channel = out_channel\n",
    "        self.base_n_filter = n_filters\n",
    "        self.attention = attention\n",
    "        self.final_activation = final_activation\n",
    "        # Up-Sampling path\n",
    "        self.up_path = nn.ModuleList()\n",
    "        self.att_path = nn.ModuleList()\n",
    "        if hemis:\n",
    "            in_channel = n_filters * 2 ** self.depth\n",
    "            self.up_path.append(UpConv(in_channel * 2, n_filters * 2 ** (self.depth - 1), drop_rate, bn_momentum,\n",
    "                                       is_2d))\n",
    "            if film_layers and film_layers[self.depth + 1]:\n",
    "                self.up_path.append(FiLMlayer(n_metadata, n_filters * 2 ** (self.depth - 1)))\n",
    "            else:\n",
    "                self.up_path.append(None)\n",
    "            # self.depth += 1\n",
    "        else:\n",
    "            in_channel = n_filters * 2 ** self.depth\n",
    "\n",
    "            self.up_path.append(UpConv(in_channel+n_filters * 2 ** (self.depth - 1)\n",
    "                                       , n_filters * 2 ** (self.depth - 1), drop_rate, bn_momentum, is_2d))#chm\n",
    "            if film_layers and film_layers[self.depth + 1]:\n",
    "                self.up_path.append(FiLMlayer(n_metadata, n_filters * 2 ** (self.depth - 1)))\n",
    "            else:\n",
    "                self.up_path.append(None)\n",
    "\n",
    "        for i in range(1, depth):\n",
    "            in_channel //= 2\n",
    "\n",
    "            self.up_path.append(UpConv(in_channel+ n_filters * 2 ** (self.depth - i - 1 + int(hemis)),\n",
    "                                       n_filters * 2 ** (self.depth - i - 1),\n",
    "                       drop_rate, bn_momentum, is_2d))\n",
    "            if film_layers and film_layers[self.depth + i + 1]:\n",
    "                self.up_path.append(FiLMlayer(n_metadata, n_filters * 2 ** (self.depth - i - 1)))\n",
    "            else:\n",
    "                self.up_path.append(None)\n",
    "\n",
    "        # Last Convolution\n",
    "        conv = nn.Conv2d if is_2d else nn.Conv3d\n",
    "        self.last_conv = conv(in_channel // 2, out_channel, kernel_size=3, padding=1)\n",
    "        self.last_film = FiLMlayer(n_metadata, self.out_channel) if film_layers and film_layers[-1] else None\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        ### ATTENTION MODULE ###\n",
    "        if self.attention:\n",
    "            \n",
    "            self.gating = UnetGridGatingSignal2(self.base_n_filter * 2**(self.depth),\n",
    "                                                self.base_n_filter * 2**(self.depth-1), kernel_size=(1, 1),\n",
    "                                                is_batchnorm=True)\n",
    "            for k in range(1,self.depth+1):\n",
    "                \n",
    "                self.att_path.append(GridAttentionBlock2D(in_channels=self.base_n_filter * 2**(self.depth-k),\n",
    "                                                        gating_channels=self.base_n_filter * 2**(self.depth-1),\n",
    "                                                        inter_channels=self.base_n_filter * 2**(self.depth-k),\n",
    "                                                        sub_sample_factor=2))\n",
    "\n",
    "\n",
    "    def forward(self, features, context=None, w_film=None):\n",
    "        x = features[-1]\n",
    "        gating = self.gating(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            if self.attention:\n",
    "                y, att = self.att_path[i](features[(self.depth-1)-i],gating)\n",
    "                x = self.up_path[2*i](x, y)\n",
    "                \n",
    "            else:\n",
    "                x = self.up_path[2*i](x, features[(self.depth-1)-i])\n",
    "            if self.up_path[2*i+1]:\n",
    "                x, w_film = self.up_path[2*i+ 1](x, context, w_film)\n",
    "                \n",
    "\n",
    "\n",
    "        # Last convolution\n",
    "        x = self.last_conv(x)\n",
    "        if self.last_film:\n",
    "            x, w_film = self.last_film(x, context, w_film)\n",
    "\n",
    "        if hasattr(self, \"final_activation\") and self.final_activation == \"softmax\":\n",
    "            preds = self.softmax(x)\n",
    "        elif hasattr(self, \"final_activation\") and self.final_activation == \"relu\":\n",
    "            preds = nn.ReLU()(x) / nn.ReLU()(x).max() if bool(nn.ReLU()(x).max()) else nn.ReLU()(x)\n",
    "            # If model multiclass\n",
    "            if preds.shape[1] > 1:\n",
    "                class_sum = preds.sum(dim=1).unsqueeze(1)\n",
    "                # Avoid division by zero\n",
    "                class_sum[class_sum == 0] = 1\n",
    "                preds /= class_sum\n",
    "        else:\n",
    "            preds = torch.sigmoid(x)\n",
    "\n",
    "        if self.out_channel > 1:\n",
    "            # Remove background class\n",
    "            preds = preds[:, 1:, ]\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "#if __name__==\"__main__\":\n",
    " #   features = [torch.rand((2, 64, 322, 322)), torch.rand((2, 128, 161, 161)),\n",
    " #              torch.rand((2, 256, 80, 80)), torch.rand((2, 512, 40, 40))]\n",
    " #   model = Decoder()\n",
    " #   print(model(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 10, 10])\n",
      "torch.Size([2, 1, 21, 21])\n",
      "torch.Size([2, 256, 21, 21])\n",
      "torch.Size([2, 256, 21, 21])\n",
      "torch.Size([2, 1, 43, 43])\n",
      "torch.Size([2, 128, 43, 43])\n",
      "torch.Size([2, 128, 43, 43])\n",
      "torch.Size([2, 1, 86, 86])\n",
      "torch.Size([2, 64, 86, 86])\n",
      "torch.Size([2, 64, 86, 86])\n",
      "the final size is equal to torch.Size([2, 1, 86, 86])\n",
      "tensor([[[[0.5289, 0.5782, 0.6105,  ..., 0.4764, 0.4914, 0.5742],\n",
      "          [0.5252, 0.5174, 0.6103,  ..., 0.4310, 0.4038, 0.5962],\n",
      "          [0.4992, 0.6070, 0.5364,  ..., 0.3027, 0.4503, 0.6104],\n",
      "          ...,\n",
      "          [0.4476, 0.3083, 0.2959,  ..., 0.4956, 0.2178, 0.3265],\n",
      "          [0.4757, 0.4349, 0.4090,  ..., 0.4034, 0.3720, 0.3553],\n",
      "          [0.5666, 0.5279, 0.5734,  ..., 0.5136, 0.4664, 0.4260]]],\n",
      "\n",
      "\n",
      "        [[[0.5342, 0.4628, 0.3959,  ..., 0.4934, 0.4997, 0.5142],\n",
      "          [0.6124, 0.5599, 0.5561,  ..., 0.5135, 0.4738, 0.5327],\n",
      "          [0.5291, 0.5371, 0.4802,  ..., 0.5075, 0.5027, 0.5677],\n",
      "          ...,\n",
      "          [0.5433, 0.5759, 0.4579,  ..., 0.5425, 0.4860, 0.5405],\n",
      "          [0.5952, 0.5146, 0.4268,  ..., 0.4938, 0.4766, 0.5673],\n",
      "          [0.5603, 0.5415, 0.5145,  ..., 0.5496, 0.5659, 0.5597]]]],\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Unet(Module):\n",
    "    \"\"\"A reference U-Net model.\n",
    "\n",
    "    .. seealso::\n",
    "        Ronneberger, O., et al (2015). U-Net: Convolutional\n",
    "        Networks for Biomedical Image Segmentation\n",
    "        ArXiv link: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    Args:\n",
    "        in_channel (int): Number of channels in the input image.\n",
    "        out_channel (int): Number of channels in the output image.\n",
    "        depth (int): Number of down convolutions minus bottom down convolution.\n",
    "        drop_rate (float): Probability of dropout.\n",
    "        bn_momentum (float): Batch normalization momentum.\n",
    "        final_activation (str): Choice of final activation between \"sigmoid\", \"relu\" and \"softmax\".\n",
    "        is_2d (bool): Indicates dimensionality of model: True for 2D convolutions, False for 3D convolutions.\n",
    "        n_filters (int):  Number of base filters in the U-Net.\n",
    "        **kwargs:\n",
    "\n",
    "    Attributes:\n",
    "        encoder (Encoder): U-Net encoder.\n",
    "        decoder (Decoder): U-net decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel=1, out_channel=1, depth=3, drop_rate=0.4, bn_momentum=0.1, final_activation='sigmoid',\n",
    "                 is_2d=True, n_filters=64, **kwargs):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        # Encoder path\n",
    "        self.encoder = Encoder(in_channel=1, depth=3, drop_rate=0.4, bn_momentum=0.1, n_metadata=None, film_layers=None,\n",
    "                 is_2d=True, n_filters=64)\n",
    "\n",
    "        # Decoder path\n",
    "        self.decoder = Decoder(out_channel=1, depth=3, drop_rate=0.4, bn_momentum=0.1,\n",
    "                 n_metadata=None, film_layers=None, hemis=False, final_activation=\"sigmoid\", is_2d=True,\n",
    "                 n_filters=64, attention=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features, _ = self.encoder(x)\n",
    "        preds = self.decoder(features)\n",
    "        \n",
    "        print(f\"the final size is equal to {preds.size()}\")\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    image = torch.rand((2,1,86,86)) # batch, Channle, W, H as input\n",
    "    model = Unet()\n",
    "    print(model(image))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
